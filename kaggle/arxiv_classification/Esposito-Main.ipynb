{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arxiv Paper Classification\n",
    "### Matteo Esposito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random, unicodedata, re, sys\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_training_set(df, ratio=0.7):\n",
    "    \"\"\"Splits training set into train and validation at a ratio of 70/30\"\"\"\n",
    "    df.sample(frac=1)\n",
    "    train = df[:int(ratio*df.shape[0])]\n",
    "    validation = df[int(ratio*df.shape[0]):]\n",
    "    return train, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleanup\n",
    "def clean(df, t):\n",
    "    df[t] = df[t].apply(lambda x : re.sub('[0-9]', '', x))\n",
    "    df[t] = df[t].apply(lambda x : re.sub('\\[[^]]*\\]', '', x))\n",
    "    df[t] = df[t].apply(lambda x : re.sub(\"<$\\.*?>\", '', x))\n",
    "    df[t] = df[t].apply(lambda x : re.sub(\"\\n\", \" \", x))\n",
    "    df[t] = df[t].apply(lambda x : x.lower())\n",
    "    df[t] = df[t].apply(lambda x : x.strip())\n",
    "    df[t] = df[t].apply(lambda x : re.sub(\"[^A-Za-z0-9]+\", \" \", x))\n",
    "    \n",
    "    # remove stop words\n",
    "    stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"ain\", \"all\", \"am\", \"an\", \n",
    "                 \"and\", \"any\", \"are\", \"aren\", \"aren't\", \"as\", \"at\", \"be\", \"because\", \"been\", \n",
    "                 \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\", \"couldn\", \n",
    "                 \"couldn't\", \"d\", \"did\", \"didn\", \"didn't\", \"do\", \"does\", \"doesn\", \"doesn't\", \n",
    "                 \"doing\", \"don\", \"don't\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \n",
    "                 \"further\", \"had\", \"hadn\", \"hadn't\", \"has\", \"hasn\", \"hasn't\", \"have\", \"haven\", \n",
    "                 \"haven't\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \n",
    "                 \"his\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"isn\", \"isn't\", \"it\", \"it's\", \"its\", \n",
    "                 \"itself\", \"just\", \"ll\", \"m\", \"ma\", \"me\", \"mightn\", \"mightn't\", \"more\", \"most\", \n",
    "                 \"mustn\", \"mustn't\", \"my\", \"myself\", \"needn\", \"needn't\", \"no\", \"nor\", \"not\", \"now\", \n",
    "                 \"o\", \"of\", \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \n",
    "                 \"out\", \"over\", \"own\", \"re\", \"s\", \"same\", \"shan\", \"shan't\", \"she\", \"she's\", \"should\", \n",
    "                 \"should've\", \"shouldn\", \"shouldn't\", \"so\", \"some\", \"such\", \"t\", \"than\", \"that\", \n",
    "                 \"that'll\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \n",
    "                 \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"ve\", \n",
    "                 \"very\", \"was\", \"wasn\", \"wasn't\", \"we\", \"were\", \"weren\", \"weren't\", \"what\", \"when\", \n",
    "                 \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\", \"will\", \"with\", \"won\", \"won't\", \n",
    "                 \"wouldn\", \"wouldn't\", \"y\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \n",
    "                 \"yours\", \"yourself\", \"yourselves\", \"could\", \"he'd\", \"he'll\", \"he's\", \"here's\", \n",
    "                 \"how's\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"let's\", \"ought\", \"she'd\", \"she'll\", \"that's\", \n",
    "                 \"there's\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"we'd\", \"we'll\", \"we're\", \"we've\", \n",
    "                 \"what's\", \"when's\", \"where's\", \"who's\", \"why's\", \"would\"]\n",
    "    df[t] = df[t].apply(lambda text: [\"\".join(w) for w in text.split(\" \") if w not in stopwords])\n",
    "    \n",
    "    # Convert list back to string\n",
    "    newabs = [' '.join(map(str, abstract_list)) for abstract_list in df[t]]\n",
    "    df[t] = newabs\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 - Naive Bayes Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_classification(out, train, test):\n",
    "    random_submission = pd.DataFrame({\"Id\": [], \"Category\": []})\n",
    "    random_submission['Id'] = test['Id']\n",
    "    random_submission['Category'] = np.random.choice(train['Category'].unique(), test.shape[0])\n",
    "    random_submission.to_csv(out, index=False)\n",
    "    print(f\"Random classification done and output to {out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFormatter():\n",
    "    def __init__(self, train, test, validation):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.validation = validation\n",
    "        self.vocab = []\n",
    "        \n",
    "    def get_vocab(self):\n",
    "        for wordstring in self.train:\n",
    "            for word in wordstring.split(\" \"):\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab.append(word)\n",
    "        \n",
    "    def create_bow(self):\n",
    "        returnarrs = []\n",
    "        for i, df in enumerate([self.train, self.test, self.validation]):\n",
    "            X_bow = []\n",
    "            for i in range(df.shape[0]):\n",
    "                bow = {}\n",
    "                abstract = df.iloc[i].split()\n",
    "\n",
    "                # Initialize\n",
    "                for word in self.vocab:\n",
    "                    bow[word] = 0\n",
    "                    \n",
    "                # Populate with 1 in case where the word is in the provided test case.\n",
    "                for word in abstract:\n",
    "                    if word in bow.keys():\n",
    "                        bow[word] = 1\n",
    "                        \n",
    "                X_bow.append(bow)\n",
    "                \n",
    "            for i in range(df.shape[0]):\n",
    "                binary_abstract = X_bow[i]\n",
    "                X_bow[i] = list(binary_abstract.values()) # Some sort of a conversion\n",
    "                \n",
    "            returnarrs.append(np.array(X_bow))\n",
    "        \n",
    "        return returnarrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliNB():\n",
    "    def fit(self, X, y, alpha=1):\n",
    "        classes = np.unique(y)\n",
    "        self.n_classes = len(classes)\n",
    "        self.n_features = X.shape[1]\n",
    "        self.likelihoods = np.zeros((self.n_classes, self.n_features))\n",
    "        for cat_idx, cat in enumerate(classes):\n",
    "            examples_in_class = X[y==cat].shape[0]\n",
    "            self.likelihoods[cat_idx] = (X[y==cat].sum(axis=0) + alpha*np.ones(self.n_features)) / (examples_in_class + alpha*self.n_classes)\n",
    "            \n",
    "    def get_posteriors(self, test_case):\n",
    "        ones = np.ones((self.n_classes, self.n_features))\n",
    "        return np.prod(self.likelihoods * test_case + (ones - self.likelihoods) * (ones - test_case), axis=1)\n",
    "    \n",
    "    def predict_single_case(self, test_case):\n",
    "        return np.argmax(self.get_posteriors(test_case))\n",
    "    \n",
    "    def predict_matrix(self, X):\n",
    "        return np.apply_along_axis(self.predict_single_case, 1, X)\n",
    "    \n",
    "    def error_rate(self, y_pred, y_true):\n",
    "        er = round(1 - sum(y_pred == y_true)/y_pred.shape[0], 2)\n",
    "        return er"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Get command line args\n",
    "    ALGO = sys.argv[1].lower()\n",
    "    TRAIN = sys.argv[2]\n",
    "    TEST = sys.argv[3]\n",
    "    OUTPATH = sys.argv[4]\n",
    "\n",
    "    print(f\"Algo: {ALGO}\\nTrain: {TRAIN}\\nTest: {TEST}\\nOutpath: {OUTPATH}\\n\")\n",
    "    \n",
    "    # Get data and split it\n",
    "    train_full = pd.read_csv(TRAIN)\n",
    "    test = pd.read_csv(TEST)\n",
    "    train, validation = split_training_set(train_full)\n",
    "\n",
    "    print(\">> Data read and split\")\n",
    "    \n",
    "    # Clean/preprocess data.\n",
    "    train = clean(train, 'Abstract')\n",
    "    validation = clean(validation, 'Abstract')\n",
    "    test = clean(test, 'Abstract')\n",
    "    \n",
    "    print(\">> Data processed\")\n",
    "    \n",
    "    # Proceed with classification\n",
    "    if ALGO == \"random\":\n",
    "        \n",
    "        # Run random classifier.\n",
    "        print(\">> Running Random Classifier\")\n",
    "        random_classification(train=train, test=test, out=OUTPATH)\n",
    "        \n",
    "    \n",
    "    # Run Naive Bayes Classifier\n",
    "    elif ALGO == \"nbbernoulli\":\n",
    "        \n",
    "        print(\">> Running Bernoulli Naive Bayes Classifier\")\n",
    "        # Format data into acceptable format (binary)\n",
    "        d = DataFormatter(train=train['Abstract'], test=test['Abstract'], validation=validation['Abstract'])\n",
    "        d.get_vocab()\n",
    "        table_output = d.create_bow()\n",
    "        train_abstracts, test_abstracts, validation_abstracts = (table_output[0], table_output[1], table_output[2])\n",
    "\n",
    "        # Create classifier\n",
    "        b = BernoulliNB()\n",
    "\n",
    "        # Get predictions and map integer output to string categories.\n",
    "        cat_mapping = {\n",
    "            0:'astro-ph',\n",
    "            1:'astro-ph.CO',\n",
    "            2:'astro-ph.GA',\n",
    "            3:'astro-ph.SR',\n",
    "            4:'cond-mat.mes-hall',\n",
    "            5:'cond-mat.mtrl-sci',\n",
    "            6:'cs.LG',\n",
    "            7:'gr-qc',\n",
    "            8:'hep-ph',\n",
    "            9:'hep-th',\n",
    "            10:'math.AP',\n",
    "            11:'math.CO',\n",
    "            12:'physics.optics',\n",
    "            13:'quant-ph',\n",
    "            14:'stat.ML'\n",
    "        }\n",
    "\n",
    "        ALPHASTAR = 0.3\n",
    "        b.fit(X=train_abstracts, y=train['Category'], alpha=ALPHASTAR)        \n",
    "        print(\">> Training done. Generating submission\")\n",
    "    \n",
    "        # Map predictions and output submission file.\n",
    "        y_test = b.predict_matrix(test_abstracts)\n",
    "        y_test = np.vectorize(cat_mapping.get)(y_test)\n",
    "        test['Category'] = y_test\n",
    "        test.drop(columns=[\"Abstract\"], inplace=True)\n",
    "        test.to_csv(OUTPATH, index=False) ## LEADERBOARD SCORE: 0.76822 w/ 0.3 (FULL DATA: 0.77755 w/ 0.3)\n",
    "        print(f\"Bernoulli NB classification done and output to {OUTPATH}\")\n",
    "    else:\n",
    "        print(\"INVALID algorithm. Choose from ['random', 'nbbernoulli']\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Code to produce comparison plots.\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# alpha_errors = {}\n",
    "# alpha_errors[a] = np.mean(y_validation == validation['Category'])\n",
    "\n",
    "# # Organise data into dataframes.\n",
    "# scores = {\"Alpha\": list(alpha_errors.keys()) + list(alpha_errors.keys()), \n",
    "#           \"Accuracy\": list(alpha_errors.values()) + [0.7733333333333333, 0.7813333333333333, 0.7893333333333333, 0.7933333333333333, 0.792, 0.7933333333333333, 0.792], \n",
    "#           \"Source\": [\"Bernoulli\", \"Bernoulli\", \"Bernoulli\", \"Bernoulli\", \"Bernoulli\", \"Bernoulli\", \"Bernoulli\", \n",
    "#                      \"Multinomial\", \"Multinomial\", \"Multinomial\", \"Multinomial\", \"Multinomial\", \"Multinomial\", \"Multinomial\"]}\n",
    "# df = pd.DataFrame(scores)\n",
    "\n",
    "# # Generate graph\n",
    "# plt.figure(figsize=(10, 7))\n",
    "# plt.title('Validation Set Prediction Accuracy of Naive Bayes Classification on Arxiv Article Abstracts\\n Using 70/30 Train/Validation Split')\n",
    "# sns.lineplot(x=\"Alpha\", y=\"Accuracy\", hue=\"Source\", data=df)\n",
    "# plt.savefig('compare.png')\n",
    "# plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mila3.7",
   "language": "python",
   "name": "mila3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
