{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random, unicodedata, re\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Matteo/projects/fundamentals-of-ml/kaggle/arxiv_classification/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"Loads data from hardcoded directory\"\"\"\n",
    "    train = pd.read_csv(\"~/projects/fundamentals-of-ml/kaggle/arxiv_classification/data/train.csv\")\n",
    "    test = pd.read_csv(\"~/projects/fundamentals-of-ml/kaggle/arxiv_classification/data/test.csv\")\n",
    "    return train, test\n",
    "\n",
    "def split_training_set(df, ratio=0.7):\n",
    "    \"\"\"Splits training set into train and validation at a ratio of 70/30\"\"\"\n",
    "    df.sample(frac=1)\n",
    "    train = df[:int(ratio*df.shape[0])]\n",
    "    validation = df[int(ratio*df.shape[0]):]\n",
    "    return train, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleanup\n",
    "def process(df, t):\n",
    "    df[t] = df[t].apply(lambda x : re.sub('[0-9]', '', x))\n",
    "    df[t] = df[t].apply(lambda x : re.sub('\\[[^]]*\\]', '', x))\n",
    "    df[t] = df[t].apply(lambda x : re.sub(\"<$\\.*?>\", '', x))\n",
    "    df[t] = df[t].apply(lambda x : re.sub('(\\${1,2})(?:(?!\\1)[\\s\\S])*\\1', \"\", x))\n",
    "    df[t] = df[t].apply(lambda x : re.sub(\"\\n\", \" \", x))\n",
    "    df[t] = df[t].apply(lambda x : x.lower())\n",
    "    df[t] = df[t].apply(lambda x : x.strip())\n",
    "    \n",
    "    \n",
    "    # remove stop words\n",
    "    stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"ain\", \"all\", \"am\", \"an\", \n",
    "                 \"and\", \"any\", \"are\", \"aren\", \"aren't\", \"as\", \"at\", \"be\", \"because\", \"been\", \n",
    "                 \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\", \"couldn\", \n",
    "                 \"couldn't\", \"d\", \"did\", \"didn\", \"didn't\", \"do\", \"does\", \"doesn\", \"doesn't\", \n",
    "                 \"doing\", \"don\", \"don't\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \n",
    "                 \"further\", \"had\", \"hadn\", \"hadn't\", \"has\", \"hasn\", \"hasn't\", \"have\", \"haven\", \n",
    "                 \"haven't\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \n",
    "                 \"his\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"isn\", \"isn't\", \"it\", \"it's\", \"its\", \n",
    "                 \"itself\", \"just\", \"ll\", \"m\", \"ma\", \"me\", \"mightn\", \"mightn't\", \"more\", \"most\", \n",
    "                 \"mustn\", \"mustn't\", \"my\", \"myself\", \"needn\", \"needn't\", \"no\", \"nor\", \"not\", \"now\", \n",
    "                 \"o\", \"of\", \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \n",
    "                 \"out\", \"over\", \"own\", \"re\", \"s\", \"same\", \"shan\", \"shan't\", \"she\", \"she's\", \"should\", \n",
    "                 \"should've\", \"shouldn\", \"shouldn't\", \"so\", \"some\", \"such\", \"t\", \"than\", \"that\", \n",
    "                 \"that'll\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \n",
    "                 \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"ve\", \n",
    "                 \"very\", \"was\", \"wasn\", \"wasn't\", \"we\", \"were\", \"weren\", \"weren't\", \"what\", \"when\", \n",
    "                 \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\", \"will\", \"with\", \"won\", \"won't\", \n",
    "                 \"wouldn\", \"wouldn't\", \"y\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \n",
    "                 \"yours\", \"yourself\", \"yourselves\", \"could\", \"he'd\", \"he'll\", \"he's\", \"here's\", \n",
    "                 \"how's\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"let's\", \"ought\", \"she'd\", \"she'll\", \"that's\", \n",
    "                 \"there's\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"we'd\", \"we'll\", \"we're\", \"we've\", \n",
    "                 \"what's\", \"when's\", \"where's\", \"who's\", \"why's\", \"would\"]\n",
    "    df[t] = df[t].apply(lambda text: [\"\".join(w) for w in text.split(\" \") if w not in stopwords])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBBernoulliClassifier:\n",
    "    \n",
    "    def __init__(self, X_train, y_train, X_test, y_test):\n",
    "        \n",
    "        # Data related attributes\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "        # Other useful attributes\n",
    "        self.bin_counts = {}\n",
    "        self.vocab = []\n",
    "        self.priors = {}\n",
    "    \n",
    "    def get_vocabulary(self):\n",
    "        for wordstring in self.X_train['Abstract']:\n",
    "            for word in wordstring:\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab.append(word)\n",
    "    \n",
    "    def get_priors(self):\n",
    "        self.unique_classes = set(self.y_train)\n",
    "        for cat in self.unique_classes:\n",
    "            self.priors[cat] = list(self.y_train).count(cat)/self.y_train.shape[0]\n",
    "    \n",
    "    def get_binary_dict(self):  \n",
    "        for cat in self.unique_classes:\n",
    "            self.bin_counts[cat] = {}\n",
    "        \n",
    "            for word in self.vocab:\n",
    "                self.bin_counts[cat].update({word: 0})\n",
    "        \n",
    "        # Here we perform a step of getting conditional probabilities which is to get the sum of indicator values of each words in each category.\n",
    "        for obs_id, wordstring, cat in zip(self.X_train['Id'], self.X_train['Abstract'], self.y_train):                \n",
    "            for word in wordstring:\n",
    "                if word in self.vocab:\n",
    "                    self.bin_counts[cat][word] += 1\n",
    "    \n",
    "    def get_conditional_probs(self, alpha=1):\n",
    "        cat_idx = {}\n",
    "        \n",
    "        # Get indices of observations in class cat.\n",
    "        for cat in self.unique_classes:\n",
    "            cat_idx[cat] = self.y_train.index[self.y_train == cat].tolist()\n",
    "        \n",
    "        # Get num and denom for p(w|C) calculation.\n",
    "        for cat, worddict in self.bin_counts.items():\n",
    "            for key in worddict.keys():\n",
    "                self.bin_counts[cat][key] = self.bin_counts[cat][key] + alpha\n",
    "                self.bin_counts[cat][key] /= (len(cat_idx[cat]) + alpha*len(self.unique_classes))\n",
    "\n",
    "    def get_unique_wordlist(self):\n",
    "        all_values_nested = [[k for k in self.bin_counts[cat].keys()] for cat in self.unique_classes]\n",
    "        self.unique_wordlist = set([i for nl in all_values_nested for i in nl])\n",
    "    \n",
    "    def get_posteriors(self, test_case, alpha):\n",
    "        \"\"\"Get posterior for a single test case.\"\"\"\n",
    "        self.posteriors = {}\n",
    "        cat_idx = {}\n",
    "        \n",
    "        # Get indices of observations in class cat.\n",
    "        for cat in self.unique_classes:\n",
    "            cat_idx[cat] = self.y_train.index[self.y_train == cat].tolist()\n",
    "        \n",
    "        self.posteriors[cat] = {}\n",
    "        for cat in self.unique_classes:\n",
    "            prob = 0\n",
    "            \n",
    "            # Get prior\n",
    "            prior = self.priors[cat]\n",
    "            \n",
    "            # Sum prior and log conditional probs\n",
    "            for test_word in test_case:\n",
    "                if test_word not in self.unique_wordlist:\n",
    "                    prob *= alpha / (len(cat_idx[cat]) + alpha*len(self.unique_classes))\n",
    "                else:\n",
    "                    prob *= self.bin_counts[cat][test_word]\n",
    "                \n",
    "            self.posteriors[cat] = prob\n",
    "    \n",
    "    def train(self):\n",
    "        nbb.get_vocabulary()\n",
    "        nbb.get_priors()\n",
    "        nbb.get_binary_dict()\n",
    "        nbb.get_conditional_probs()\n",
    "    \n",
    "    def predict(self, X, alpha):\n",
    "        preds = []\n",
    "        self.get_unique_wordlist()\n",
    "        for sample in X[\"Abstract\"]:\n",
    "            self.get_posteriors(sample, alpha)\n",
    "            preds.append(max(self.posteriors, key=self.posteriors.get))\n",
    "        X[\"Pred_Category\"] = preds\n",
    "    \n",
    "    def error_rate(self, y_pred, y_true):\n",
    "        er = round(1 - sum(y_pred == y_true)/len(y_pred), 9)\n",
    "        print(f\"Error Rate = {er}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.6 s, sys: 221 ms, total: 12.8 s\n",
      "Wall time: 13.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train, test = load_data()\n",
    "train, validation = split_training_set(train)\n",
    "\n",
    "train = process(train, 'Abstract')\n",
    "validation = process(validation, 'Abstract')\n",
    "test = process(test, 'Abstract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 39s, sys: 1.47 s, total: 3min 41s\n",
      "Wall time: 3min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nbb = NBBernoulliClassifier(X_train=train[[\"Id\", \"Abstract\"]], y_train=train[\"Category\"],\n",
    "                            X_test=validation[[\"Id\", \"Abstract\"]], y_test=validation[\"Category\"])\n",
    "nbb.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.001\n",
      "Error Rate = 0.488444444\n",
      "alpha: 0.01\n",
      "Error Rate = 0.488444444\n",
      "alpha: 0.05\n",
      "Error Rate = 0.488444444\n",
      "alpha: 0.1\n",
      "Error Rate = 0.488444444\n",
      "alpha: 0.3\n",
      "Error Rate = 0.488444444\n",
      "alpha: 0.5\n",
      "Error Rate = 0.488444444\n"
     ]
    }
   ],
   "source": [
    "for a in [0.001, 0.01, 0.05, 0.1, 0.3, 0.5]:\n",
    "    nbb.predict(nbb.X_test, alpha=a)\n",
    "    print(f\"alpha: {a}\")\n",
    "    nbb.error_rate(y_pred=nbb.X_test['Pred_Category'], y_true=nbb.y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mila3.7",
   "language": "python",
   "name": "mila3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
