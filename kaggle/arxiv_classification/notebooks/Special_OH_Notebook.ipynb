{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "#import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "#import gc\n",
    "import re, unicodedata\n",
    "#from collections import defaultdict\n",
    "import string\n",
    "#import tqdm\n",
    "\n",
    "\n",
    "# this will not be used,\n",
    "#from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "#from sklearn.naive_bayes import MultinomialNB, BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Matteo/projects/fundamentals-of-ml/kaggle/arxiv_classification/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train = pd.read_csv(\"~/projects/fundamentals-of-ml/kaggle/arxiv_classification/data/train.csv\")\n",
    "    test = pd.read_csv(\"~/projects/fundamentals-of-ml/kaggle/arxiv_classification/data/test.csv\")\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return ''.join(new_words)\n",
    "\n",
    "def process(df, t):\n",
    "    df[t] = df[t].apply(lambda x : x.lower())\n",
    "    #train['Abstract'] = train['Abstract'].apply(lambda x : remove_punctuation(x))\n",
    "    df[t] = df[t].apply(lambda x : x.strip())\n",
    "    df[t] = df[t].apply(lambda x : re.sub('\\n', ' ', x))\n",
    "    df[t] = df[t].apply(lambda x : re.sub('\\[[^]]*\\]', '', x))\n",
    "    df[t] = df[t].apply(lambda x : re.sub(\"<.*?>\", \" \", x))\n",
    "    df[t] = df[t].apply(lambda x : remove_non_ascii(x))\n",
    "    print(df.head())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id                                           Abstract           Category\n",
      "0   0  the energy released in a solar flare is partit...           astro-ph\n",
      "1   1  in light of current atmospheric neutrino oscil...             hep-ph\n",
      "2   2  we consider the following basic learning task:...              cs.LG\n",
      "3   3  in this paper, we characterise the family of f...            math.CO\n",
      "4   4  the control of condensed matter systems out of...  cond-mat.mes-hall\n",
      "   Id                                           Abstract\n",
      "0   0  we describe ways to define and calculate $l_1$...\n",
      "1   1  the progenitor systems of type-ia supernovae (...\n",
      "2   2  omegawhite is a wide-field, high cadence, syno...\n",
      "3   3  given $n \\geq 2$ and $1<p<n$, we consider the ...\n",
      "4   4  the motivation of this work is to improve the ...\n"
     ]
    }
   ],
   "source": [
    "train, test = load_data()\n",
    "train = process(train, 'Abstract')\n",
    "test = process(test, 'Abstract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliVectorizer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vocab = []\n",
    "        self.vocab_counter = {}\n",
    "        \n",
    "    def build_vocab(self, data):\n",
    "        for document in data:\n",
    "            for word in document.split(' '):\n",
    "                if word in self.vocab:\n",
    "                    self.vocab_counter[str(word)] += 1\n",
    "                else:\n",
    "                    self.vocab.append(str(word))\n",
    "                    self.vocab_counter = 1\n",
    "                \n",
    "    def transform(self, data):\n",
    "        '''  this is some text\" -----> (0, 0, 0, 0, 0, 1 ...) '''\n",
    "        i = 0\n",
    "        for document in data:\n",
    "            tokens = document.split(' ') # ['my', 'name', 'is', 'kasper', 'and', 'i', 'like', 'ml']\n",
    "                                         # ['my', 'kasper', 'hello', ..................]\n",
    "                                         # (1, 1, 0, 0, )\n",
    "            bin_vect = np.zeros(len(self.vocab))\n",
    "            for word_idx in range(len(self.vocab)):\n",
    "                for e in tokens: \n",
    "                    if e == self.vocab[word_idx]:\n",
    "                        bin_vect[word_idx] = 1\n",
    "                data[i] = bin_vect   # please do something else AAAAHH\n",
    "                i += 1\n",
    "                \n",
    "        return data\n",
    "    \n",
    "    def fit_transform(self, data):\n",
    "        self.build_vocab(data)\n",
    "        return transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-99005410fa36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBernoulliVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Abstract'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Abstract'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Abstract'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Abstract'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# transform only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-c8b1ed9c5ef4>\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-c8b1ed9c5ef4>\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "B = BernoulliVectorizer()\n",
    "train['Abstract'] = B.fit_transform(train['Abstract'])\n",
    "test['Abstract'] = B.transform(test['Abstract'])        # transform only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head() # (0,0,0,0,1,0,0), (0,0,0,0,1,0,0), (0,0,0,0,1,0,0), (0,0,0,0,1,0,0), (0,0,0,0,1,0,0), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train.to_numpy(), test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliNB:\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "        pass\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    P(C_k) = number of documents of that class / number of documents\n",
    "    P(C_k | w1, w2 ,... ) \\prop P(C_k) P(w1| C_k) P(w2 | C_k) ..\n",
    "    P(wi | C_k) = number of documents of class C_k with wi / number of documents with that class\n",
    "     get all the rows of class C_k, how many of them has word wi\n",
    "     get all the rows of class C_k, how many of them has a 1 in the index of wi\n",
    "    '''\n",
    "    def fit(self, X, y):\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        n_classes = self.n_classes\n",
    "        \n",
    "        # calculate P(C_k) for all k\n",
    "        self.counts = np.zeros(n_classes)\n",
    "        for i in y:\n",
    "            self.counts[i] += 1\n",
    "        self.counts /= len(y)\n",
    "        \n",
    "        \n",
    "        # generate n_features x n_classes matrix\n",
    "        self.params = np.zeros((n_classes, X.shape[1]))\n",
    "        for idx in range(len(X)):\n",
    "            self.params[y[idx]] += X[idx]\n",
    "        self.params += self.alpha #1 # Laplace\n",
    "        class_sums = self.params.sum(axis=1) + self.alpha * self.n_classes # Laplace\n",
    "        self.params = self.params / class_sums[:, np.newaxis]\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        neg_prob = np.log(1 - self.params)\n",
    "        # Compute  neg_prob · (1 - X).T  as  ∑neg_prob - X · neg_prob\n",
    "        safe_sparse_dot\n",
    "        jll = np.dot(X, (np.log(self.params) - neg_prob).T)\n",
    "        \n",
    "        # posterior calc where np.log(self.counts)=prior, neg_prob.sum(axis=1)=likelihood\n",
    "        jll += np.log(self.counts) + neg_prob.sum(axis=1)\n",
    "        \n",
    "        # return max of posterior\n",
    "        return np.argmax(jll, axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
