{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class SVM:\n",
    "    def __init__(self, eta, C, niter, batch_size, verbose):\n",
    "        self.eta = eta\n",
    "        self.C = C\n",
    "        self.niter = niter\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def make_one_versus_all_labels(self, y, m):\n",
    "        \"\"\"\n",
    "        y : numpy array of shape (n,)\n",
    "        m : int (num_classes)\n",
    "        returns : numpy array of shape (n, m)\n",
    "        \"\"\"\n",
    "        out = np.ones((len(y),m)) * -1 \n",
    "        out[np.arange(len(y)),y] = 1\n",
    "        return out\n",
    "\n",
    "    def compute_loss(self, x, y):\n",
    "        \"\"\"\n",
    "        x : numpy array of shape (minibatch size, num_features)\n",
    "        y : numpy array of shape (minibatch size, num_classes)\n",
    "        returns : float\n",
    "        \"\"\"\n",
    "        reg_loss = (self.C/2) * np.sum(np.multiply(self.w,self.w))\n",
    "        hinge_slope = 2 - np.multiply(np.matmul(x,self.w),y)\n",
    "        hinge_loss = (1/x.shape[0]) * np.sum(np.maximum(0, hinge_slope)**2) \n",
    "        return hinge_loss + reg_loss\n",
    "\n",
    "\n",
    "    def compute_gradient(self, x, y):\n",
    "        \"\"\"\n",
    "        x : numpy array of shape (minibatch size, num_features)\n",
    "        y : numpy array of shape (minibatch size, num_classes)\n",
    "        returns : numpy array of shape (num_features, num_classes)\n",
    "        \"\"\"\n",
    "        reg_grad = self.C * self.w\n",
    "        \n",
    "        hinge_val_slope = 2 - np.multiply(np.matmul(x,self.w),y)\n",
    "        hinge_val = np.maximum(0,hinge_val_slope)\n",
    "        #active = (hinge_val > 0).astype(int)\n",
    "        \n",
    "#         hinge_grad = np.zeros([x.shape[1],y.shape[1]])\n",
    "#         for cls in range(y.shape[1]):\n",
    "#             hinge_grad[:,cls] = np.mean(-1*x*(y[:,cls]*hinge_val[:,cls])[:,np.newaxis], axis=0)\n",
    "\n",
    "        hinge_grad = np.matmul(x.transpose(), -1*y*hinge_val) / x.shape[0]\n",
    "        #hinge_grad = 2*hinge_grad*np.maximum(0,hinge_slope)\n",
    "        return 2 * hinge_grad + reg_grad\n",
    "\n",
    "    # Batcher function\n",
    "    def minibatch(self, iterable1, iterable2, size=1):\n",
    "        l = len(iterable1)\n",
    "        n = size\n",
    "        for ndx in range(0, l, n):\n",
    "            index2 = min(ndx + n, l)\n",
    "            yield iterable1[ndx: index2], iterable2[ndx: index2]\n",
    "\n",
    "    def infer(self, x):\n",
    "        \"\"\"\n",
    "        x : numpy array of shape (num_examples_to_infer, num_features)\n",
    "        returns : numpy array of shape (num_examples_to_infer, num_classes)\n",
    "        \"\"\"\n",
    "        infered = np.ones([x.shape[0], self.m]) * (-1)\n",
    "        distances = np.zeros([x.shape[0], self.m])\n",
    "        for clss in range(self.m):\n",
    "            #not sure whether i should use the norming by w\n",
    "            distances[:,clss] = np.matmul(x,self.w[:,clss]) #/ np.sum(self.w[:,cls]*self.w[:,cls])\n",
    "        \n",
    "        infered[np.arange(x.shape[0]), np.argmax(distances, axis=1)] = 1\n",
    "        return infered\n",
    "\n",
    "    def compute_accuracy(self, y_inferred, y):\n",
    "        \"\"\"\n",
    "        y_inferred : numpy array of shape (num_examples, num_classes)\n",
    "        y : numpy array of shape (num_examples, num_classes)\n",
    "        returns : float\n",
    "        \"\"\"\n",
    "        matching = np.mean(np.multiply(y_inferred,y), axis=1)\n",
    "        \n",
    "        return np.sum(matching==1)/y.shape[0]\n",
    "\n",
    "    def fit(self, x_train, y_train, x_test, y_test):\n",
    "        \"\"\"\n",
    "        x_train : numpy array of shape (number of training examples, num_features)\n",
    "        y_train : numpy array of shape (number of training examples, num_classes)\n",
    "        x_test : numpy array of shape (number of training examples, nujm_features)\n",
    "        y_test : numpy array of shape (number of training examples, num_classes)\n",
    "        returns : float, float, float, float\n",
    "        \"\"\"\n",
    "        self.num_features = x_train.shape[1]\n",
    "        self.m = y_train.max() + 1\n",
    "        y_train = self.make_one_versus_all_labels(y_train, self.m)\n",
    "        y_test = self.make_one_versus_all_labels(y_test, self.m)\n",
    "        self.w = np.zeros([self.num_features, self.m])\n",
    "\n",
    "        train_losses = []\n",
    "        train_accs = []\n",
    "        test_losses = []\n",
    "        test_accs = []\n",
    "\n",
    "        for iteration in range(self.niter):\n",
    "            # Train one pass through the training set\n",
    "            for x, y in self.minibatch(x_train, y_train, size=self.batch_size):\n",
    "                grad = self.compute_gradient(x, y)\n",
    "                self.w -= self.eta * grad\n",
    "\n",
    "            # Measure loss and accuracy on training set\n",
    "            train_loss = self.compute_loss(x_train, y_train)\n",
    "            y_inferred = self.infer(x_train)\n",
    "            train_accuracy = self.compute_accuracy(y_inferred, y_train)\n",
    "\n",
    "            # Measure loss and accuracy on test set\n",
    "            test_loss = self.compute_loss(x_test, y_test)\n",
    "            y_inferred = self.infer(x_test)\n",
    "            test_accuracy = self.compute_accuracy(y_inferred, y_test)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"Iteration {iteration} | Train loss {train_loss:.04f} | Train acc {train_accuracy:.04f} |\"\n",
    "                      f\" Test loss {test_loss:.04f} | Test acc {test_accuracy:.04f}\")\n",
    "\n",
    "            # Record losses, accs\n",
    "            train_losses.append(train_loss)\n",
    "            train_accs.append(train_accuracy)\n",
    "            test_losses.append(test_loss)\n",
    "            test_accs.append(test_accuracy)\n",
    "\n",
    "        return train_losses, train_accs, test_losses, test_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS FUNCTION\n",
    "def load_data():\n",
    "    # Load the data files\n",
    "    print(\"Loading data...\")\n",
    "    x_train = np.load(\"data/train_features_cifar100_reduced.npz\")[\"train_data\"]\n",
    "    x_test = np.load(\"data/test_features_cifar100_reduced.npz\")[\"test_data\"]\n",
    "    y_train = np.load(\"data/train_labels_cifar100_reduced.npz\")[\"train_label\"]\n",
    "    y_test = np.load(\"data/test_labels_cifar100_reduced.npz\")[\"test_label\"]\n",
    "\n",
    "    # normalize the data\n",
    "    mean = x_train.mean(axis=0)\n",
    "    std = x_train.std(axis=0)\n",
    "    x_train = (x_train - mean) / std\n",
    "    x_test = (x_test - mean) / std\n",
    "\n",
    "    # add implicit bias in the feature\n",
    "    x_train = np.concatenate([x_train, np.ones((x_train.shape[0], 1))], axis=1)\n",
    "    x_test = np.concatenate([x_test, np.ones((x_test.shape[0], 1))], axis=1)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Fitting the model...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    x_train, y_train, x_test, y_test = load_data()\n",
    "\n",
    "    print(\"Fitting the model...\")\n",
    "    svm = SVM(eta=0.0001, C=2, niter=200, batch_size=5000, verbose=False)\n",
    "    train_losses, train_accs, test_losses, test_accs = svm.fit(x_train, y_train, x_test, y_test)\n",
    "\n",
    "    # # to infer after training, do the following:\n",
    "    # y_inferred = svm.infer(x_test)\n",
    "\n",
    "    ## to compute the gradient or loss before training, do the following:\n",
    "    # y_train_ova = svm.make_one_versus_all_labels(y_train, 8) # one-versus-all labels\n",
    "    # svm.w = np.zeros([3073, 8])\n",
    "    # grad = svm.compute_gradient(x_train, y_train_ova)\n",
    "    # loss = svm.compute_loss(x_train, y_train_ova)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mila3.8",
   "language": "python",
   "name": "mila3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
